{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-08T09:29:55.591789Z",
     "start_time": "2023-02-08T09:29:55.589714Z"
    },
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# 端到端的多示例学习,从MRI图像+实例分割掩膜中提取特征,预测病人类型."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-08T09:29:57.297860Z",
     "start_time": "2023-02-08T09:29:55.593384Z"
    },
    "code_folding": [],
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "## import \n",
    "import argparse\n",
    "from collections import Counter\n",
    "import copy\n",
    "from glob import glob\n",
    "\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import pdb\n",
    "import random\n",
    "\n",
    "import scipy\n",
    "import SimpleITK as sitk\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report, roc_auc_score, accuracy_score, recall_score, precision_score\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, roc_curve, auc\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, sampler, Dataset\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "\n",
    "from models.Onlymask_resnet import Resmode\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-08T09:29:57.305123Z",
     "start_time": "2023-02-08T09:29:57.299337Z"
    },
    "code_folding": [
     0
    ],
    "lines_to_next_cell": 2,
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "# Training settings\n",
    "parser = argparse.ArgumentParser(description='Configurations for Mission1 Training')\n",
    "\n",
    "parser.add_argument('--max_epoch',\n",
    "                    type=int,\n",
    "                    default=100,\n",
    "                    help='maximum number of epochs to train (default: 200)')\n",
    "parser.add_argument('--lr',\n",
    "                    type=float,\n",
    "                    default=1e-4,\n",
    "                    help='learning rate (default: 0.0001)')\n",
    "parser.add_argument('--fold',\n",
    "                    type=int,\n",
    "                    default=10,\n",
    "                    help='number of folds (default: 1)')\n",
    "parser.add_argument('--results_dir',\n",
    "                    default='./results',\n",
    "                    help='results directory (default: ./results)')\n",
    "\n",
    "parser.add_argument('--log_data',\n",
    "                    action='store_true',\n",
    "                    default=False,\n",
    "                    help='log data using tensorboard')\n",
    "parser.add_argument('--opt', type=str, choices=['adam', 'sgd'], default='adam')\n",
    "parser.add_argument('--drop_out',\n",
    "                    action='store_true',\n",
    "                    default=False,\n",
    "                    help='enabel dropout (p=0.25)')\n",
    "parser.add_argument('--backbone_requires_grad',\n",
    "                    action='store_true',\n",
    "                    default=True,\n",
    "                    help='whether to train the backbone')\n",
    "parser.add_argument('--input_channel',\n",
    "                    type=int,\n",
    "                    default=5,\n",
    "                    help='number of input image channels')\n",
    "parser.add_argument('--gpu', type=str, default='1', help='GPU to use')\n",
    "args = parser.parse_args(args=[])  # args=[]\n",
    "args.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "args.n_classes = 2\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = args.gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-08T09:29:57.430336Z",
     "start_time": "2023-02-08T09:29:57.306235Z"
    },
    "code_folding": [
     13,
     43,
     61,
     68,
     77,
     81
    ],
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "# Dataset\n",
    "class M1_dataset(Dataset):\n",
    "    def __init__(self, path_list, label_list, mode, aug=0.3):\n",
    "        self.All_data = path_list\n",
    "        self.All_label = label_list\n",
    "        self.mode = mode\n",
    "        self.aug = aug\n",
    "        self.phase = ['_T1', '_T2', '_AP', '_PP', '_20 MIN']\n",
    "\n",
    "        self.index_0 = [x for x, y in list(enumerate(label_list)) if y == 0]\n",
    "        self.index_1 = [x for x, y in list(enumerate(label_list)) if y == 1]\n",
    "        \n",
    "\n",
    "    def __getitem__(self, index):\n",
    "           # transformer\n",
    "        def random_drop(img_clip, max_rate=0.2):\n",
    "            c = img_clip.shape[1]\n",
    "            d = img_clip.shape[2]\n",
    "            # 随机生成一个立方体\n",
    "            cut_c = np.random.rand(1)\n",
    "            cut_d = np.random.rand(1)\n",
    "\n",
    "            while cut_c*cut_d > max_rate:\n",
    "                if np.random.rand(1) > 0.5:\n",
    "                    cut_c = max(cut_c*0.8, 0.01)\n",
    "                else:\n",
    "                    cut_d = max(cut_d*0.8, 0.01)\n",
    "\n",
    "            drop_d1 = np.random.randint(0, d-d*cut_d+1)\n",
    "            drop_c1 = np.random.randint(0, c-c*cut_c+1)\n",
    "\n",
    "#             if np.random.rand(1)>0.5:\n",
    "            img_clip[:,int(drop_c1):int(min(drop_c1+c*cut_c, c)),\n",
    "                     int(drop_d1):int(min(drop_d1+d*cut_d, d))] = np.zeros(( 25,\n",
    "                                                                           (int(min(drop_c1+c*cut_c, c)-drop_c1)),\n",
    "                                                                           (int(min(drop_d1+d*cut_d, d)-drop_d1))))\n",
    "#             else:\n",
    "#                 img_clip[:,int(drop_c1):int(min(drop_c1+c*cut_c, c)),\n",
    "#                          int(drop_d1):int(min(drop_d1+d*cut_d, d))] = np.ones(( 5,\n",
    "#                                                                                (int(min(drop_c1+c*cut_c, c)-drop_c1)),\n",
    "#                                                                                (int(min(drop_d1+d*cut_d, d)-drop_d1))))\n",
    "            return img_clip\n",
    "\n",
    "        def random_clip(img_clip, max_rate=0.1):\n",
    "            a = img_clip.shape[0]\n",
    "            c = img_clip.shape[2]\n",
    "            d = img_clip.shape[3]\n",
    "            drop_rate1 = np.random.randint(0, max_rate*100) / 100\n",
    "            drop_rate2 = np.random.randint(0, max_rate*100) / 100\n",
    "\n",
    "            if (np.random.rand(1) > 0.5) and (a > 10):\n",
    "                img_clip = img_clip[int(a * drop_rate1):\n",
    "                                    int(a * (1 - drop_rate2)), :, :, :]\n",
    "            if (np.random.rand(1) > 0.5) and (c > 10):\n",
    "                img_clip = img_clip[:, :, int(c * drop_rate1):\n",
    "                                    int(c * (1 - drop_rate2)), :]\n",
    "            if (np.random.rand(1) > 0.5) and (d > 10):\n",
    "                img_clip = img_clip[:, :, :, int(d * drop_rate1):\n",
    "                                    int(d * (1 - drop_rate2))]\n",
    "            return img_clip\n",
    "\n",
    "        def random_flip(img_clip, flip_rate=0.5):\n",
    "            if np.random.rand(1) > flip_rate:\n",
    "                img_clip = img_clip[:, ::-1, :].copy()\n",
    "            if np.random.rand(1) > flip_rate:\n",
    "                img_clip = img_clip[:, :, ::-1].copy()\n",
    "            return img_clip\n",
    "\n",
    "        def resize_np(img, img_new):\n",
    "            a = img.shape[0]\n",
    "            c = img.shape[1]\n",
    "            a_new = img_new.shape[0]\n",
    "            c_new = img_new.shape[1]\n",
    "            img = scipy.ndimage.zoom(\n",
    "                img, (a_new/a,  c_new/c))\n",
    "            return img\n",
    "\n",
    "        def copy_channel(img, channel=0):\n",
    "            img_clip = np.concatenate(\n",
    "                [img_clip.copy(), img_clip.copy()], axis=channel)\n",
    "\n",
    "        def mix_up(img, target):\n",
    "            if target == 0:\n",
    "                index_supply = random.choice(self.index_0)\n",
    "            else:\n",
    "                index_supply = random.choice(self.index_1)\n",
    "            name_file_supply = self.All_data[index_supply]\n",
    "\n",
    "            # 读取新的数据\n",
    "            supply_img = np.zeros((25, 32, 32))\n",
    "            n = 0\n",
    "\n",
    "            for phase_id in range(5):\n",
    "                channel_name = name_file_supply[:-4] + self.phase[phase_id] + '.npy'\n",
    "                img_channel = np.load(channel_name)\n",
    "\n",
    "                img1 = np.max(img_channel, axis=0)\n",
    "                img2 = np.mean(img_channel, axis=0)\n",
    "                img3 = np.min(img_channel, axis=0)\n",
    "                img4 = img1-img3\n",
    "                img5 = np.median(img_channel, axis=0)\n",
    "\n",
    "                for img_fill in [img1, img2, img3, img4, img5]:\n",
    "                    img_fill = img_fill/(img1.max()+0.01)\n",
    "                    img_fill = resize_np(img_fill, np.zeros((32, 32)))\n",
    "                    supply_img[n, :, :] = img_fill\n",
    "                    n+=1\n",
    "            \n",
    "            ratio = np.random.rand(1)/2+0.5\n",
    "            img = img*ratio + supply_img*(1-ratio)\n",
    "            return img\n",
    "\n",
    "\n",
    "        name_file, target = self.All_data[index], self.All_label[index]\n",
    "        output_img= np.zeros((25, 32, 32))\n",
    "        n = 0\n",
    "        \n",
    "        for phase_id in range(5):\n",
    "            channel_name = name_file[:-4] + self.phase[phase_id] + '.npy'\n",
    "            img_channel = np.load(channel_name)\n",
    "            \n",
    "            img1 = np.max(img_channel, axis=0)\n",
    "            img2 = np.mean(img_channel, axis=0)\n",
    "            img3 = np.min(img_channel, axis=0)\n",
    "            img4 = img1-img3\n",
    "            img5 = np.median(img_channel, axis=0)\n",
    "\n",
    "            for img_fill in [img1, img2, img3, img4, img5]:\n",
    "                img_fill = img_fill/(img1.max()+0.01)\n",
    "                img_fill = resize_np(img_fill, np.zeros((32, 32)))\n",
    "                output_img[n, :, :] = img_fill\n",
    "                n+=1\n",
    "\n",
    "        if self.mode =='train':\n",
    "            # 随机翻转\n",
    "            if np.random.rand(1)<1/3:\n",
    "                output_img = mix_up(output_img, target=target)\n",
    "            elif np.random.rand(1)<1/2:\n",
    "                output_img = random_drop(output_img, max_rate=self.aug)\n",
    "            output_img = random_flip(output_img, flip_rate=0.5)\n",
    "\n",
    "\n",
    "        return output_img, target, name_file[:-4]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.All_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-08T09:29:57.443279Z",
     "start_time": "2023-02-08T09:29:57.431680Z"
    },
    "code_folding": [
     1,
     33,
     77
    ]
   },
   "outputs": [],
   "source": [
    "# 获取样本比例\n",
    "def get_weights(train_label_list):\n",
    "    Count_sample = dict(Counter(train_label_list))\n",
    "    N = float(len(train_label_list))\n",
    "    weight_per_class = [1 / Count_sample[0], 1 / Count_sample[1]]\n",
    "\n",
    "    weight = [0] * int(N)\n",
    "    for idx in range(len(train_label_list)):\n",
    "        y = train_label_list[idx]\n",
    "        weight[idx] = weight_per_class[y]\n",
    "    weights = torch.DoubleTensor(weight)\n",
    "    return weights\n",
    "\n",
    "\n",
    "def train_simple(epoch, net, optimizer, dataloader):\n",
    "    num_iter = (len(dataloader.dataset) // dataloader.batch_size) + 1\n",
    "\n",
    "    for batch_idx, (inputs, labels, _) in enumerate(dataloader):\n",
    "        net.train()\n",
    "        inputs, labels = inputs.to(torch.float32).cuda(), labels.cuda()\n",
    "        optimizer.zero_grad()   # reset gradient\n",
    "        outputs, _ = net(inputs)\n",
    "         \n",
    "        L = F.cross_entropy(outputs, labels)\n",
    "        L.backward()\n",
    "        optimizer.step()\n",
    "                \n",
    "        if ((batch_idx + 1) % 10 == 0) or (batch_idx + 1  == num_iter):\n",
    "            print('| Epoch [%3d/%3d] Iter[%3d/%3d]\\t CE-loss: %.4f' %\n",
    "                  (epoch, args.max_epoch, batch_idx + 1, num_iter, L.item()))\n",
    "            \n",
    "\n",
    "\n",
    "def Net_val(epoch, net, dataloader):\n",
    "    net.eval()\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    loss_x = 0\n",
    "    n = 0\n",
    "    \n",
    "    y_true = np.zeros(len(dataloader.dataset))\n",
    "    y_hat = np.zeros(len(dataloader.dataset))\n",
    "    y_score = np.zeros((len(dataloader.dataset),2))\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for _, (inputs, targets, _) in enumerate(dataloader):\n",
    "\n",
    "            inputs = inputs.to(torch.float32).cuda()\n",
    "            outputs, _ = net(inputs)\n",
    "\n",
    "            predicted = torch.argmax(outputs, -1) \n",
    "            loss = F.cross_entropy(outputs, targets.cuda())\n",
    "            loss_x += loss.item()\n",
    "            \n",
    "            for b in range(targets.size(0)):\n",
    "                y_true[n] = targets[b]\n",
    "                y_hat[n] = predicted[b]\n",
    "                y_score[n,:] = np.array(outputs.cpu())[b,:]\n",
    "                n += 1       \n",
    "            \n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets.cuda()).cpu().sum().item()\n",
    "\n",
    "    acc = 100. * correct / total\n",
    "    loss_x = loss_x/total\n",
    "            \n",
    "    # 计算auc        \n",
    "    roc_auc = metrics.roc_auc_score(y_true, y_score[:,1])\n",
    "    spe_auc = metrics.roc_auc_score(1-y_true, y_score[:,0])\n",
    "    \n",
    "    print(\"\\n| Test Epoch #%d\\t Accuracy: %.2f%%\\t Loss: %.2f\\t AUC: %.3f\\t SPE: %.3f\\n\" %\n",
    "                  (epoch, acc, loss_x, roc_auc, spe_auc))\n",
    "    \n",
    "    return (acc, loss_x, roc_auc, spe_auc), (y_true, y_hat, y_score)\n",
    "\n",
    "\n",
    "def bootstrap_score(func, y, pred, classes, bootstraps = 100, fold_size = 1000):\n",
    "    '''\n",
    "    func: callable score function\n",
    "    y不是onehot\n",
    "    '''\n",
    "    statistics = np.zeros((len(classes), bootstraps))\n",
    "\n",
    "    for k,c in enumerate(classes):\n",
    "        df = pd.DataFrame(columns=['y', 'pred'])\n",
    "        df.loc[:, 'y'] = y\n",
    "        try:\n",
    "            df.loc[:, 'pred'] = pred[:, -1]\n",
    "        except:\n",
    "            df.loc[:, 'pred'] = pred[:]\n",
    "\n",
    "        df_pos = df[df.y == 1]\n",
    "        df_neg = df[df.y == 0]\n",
    "        prevalence = len(df_pos) / len(df)\n",
    "        for i in range(bootstraps):\n",
    "            pos_sample = df_pos.sample(n = int(fold_size * prevalence), replace=True)\n",
    "            neg_sample = df_neg.sample(n = int(fold_size * (1-prevalence)), replace=True)\n",
    "\n",
    "            y_sample = np.concatenate([pos_sample.y.values, neg_sample.y.values])\n",
    "            pred_sample = np.concatenate([pos_sample.pred.values, neg_sample.pred.values])\n",
    "            score = func(y_sample, pred_sample)\n",
    "            statistics[k][i] = score\n",
    "    return statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-08T09:41:25.336202Z",
     "start_time": "2023-02-08T09:29:57.444470Z"
    },
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building dataloader: training: 746, testing: 83\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Epoch [  0/100] Iter[ 10/ 47]\t CE-loss: 0.7036\n",
      "| Epoch [  0/100] Iter[ 20/ 47]\t CE-loss: 0.7116\n",
      "| Epoch [  0/100] Iter[ 30/ 47]\t CE-loss: 0.7428\n",
      "| Epoch [  0/100] Iter[ 40/ 47]\t CE-loss: 0.7268\n",
      "| Epoch [  0/100] Iter[ 47/ 47]\t CE-loss: 0.6675\n",
      "Now evaluating on fold 0\n",
      "\n",
      "| Test Epoch #0\t Accuracy: 57.10%\t Loss: 0.04\t AUC: 0.753\t SPE: 0.738\n",
      "\n",
      "\n",
      "| Test Epoch #0\t Accuracy: 57.83%\t Loss: 0.05\t AUC: 0.773\t SPE: 0.770\n",
      "\n",
      "tensor([0.5000, 0.5000, 0.5000, 0.5000, 0.5000], device='cuda:0')\n",
      "saving model\n",
      "Now fold=0, best_acc=57.83132530120482， best_auc=0.7726190476190475, best_loss=0.04798396021486765\n",
      "| Epoch [  1/100] Iter[ 10/ 47]\t CE-loss: 0.7276\n",
      "| Epoch [  1/100] Iter[ 20/ 47]\t CE-loss: 0.5258\n",
      "| Epoch [  1/100] Iter[ 30/ 47]\t CE-loss: 0.6341\n",
      "| Epoch [  1/100] Iter[ 40/ 47]\t CE-loss: 0.5270\n",
      "| Epoch [  1/100] Iter[ 47/ 47]\t CE-loss: 0.6543\n",
      "Now evaluating on fold 0\n",
      "\n",
      "| Test Epoch #1\t Accuracy: 76.54%\t Loss: 0.03\t AUC: 0.848\t SPE: 0.848\n",
      "\n",
      "\n",
      "| Test Epoch #1\t Accuracy: 75.90%\t Loss: 0.04\t AUC: 0.838\t SPE: 0.833\n",
      "\n",
      "tensor([0.5000, 0.5000, 0.5000, 0.5000, 0.5000], device='cuda:0')\n",
      "saving model\n",
      "saving model\n",
      "saving model\n",
      "Now fold=0, best_acc=75.90361445783132， best_auc=0.838095238095238, best_loss=0.0395195599061897\n",
      "| Epoch [  2/100] Iter[ 10/ 47]\t CE-loss: 0.4612\n",
      "| Epoch [  2/100] Iter[ 20/ 47]\t CE-loss: 0.5784\n",
      "| Epoch [  2/100] Iter[ 30/ 47]\t CE-loss: 0.5842\n",
      "| Epoch [  2/100] Iter[ 40/ 47]\t CE-loss: 1.0507\n",
      "| Epoch [  2/100] Iter[ 47/ 47]\t CE-loss: 0.6937\n",
      "Now evaluating on fold 0\n",
      "\n",
      "| Test Epoch #2\t Accuracy: 75.47%\t Loss: 0.03\t AUC: 0.848\t SPE: 0.851\n",
      "\n",
      "\n",
      "| Test Epoch #2\t Accuracy: 74.70%\t Loss: 0.04\t AUC: 0.804\t SPE: 0.803\n",
      "\n",
      "tensor([0.5000, 0.5000, 0.5000, 0.5000, 0.5000], device='cuda:0')\n",
      "saving model\n",
      "Now fold=0, best_acc=75.90361445783132， best_auc=0.838095238095238, best_loss=0.03710524971226612\n",
      "| Epoch [  3/100] Iter[ 10/ 47]\t CE-loss: 0.5705\n",
      "| Epoch [  3/100] Iter[ 20/ 47]\t CE-loss: 0.4421\n",
      "| Epoch [  3/100] Iter[ 30/ 47]\t CE-loss: 0.7847\n",
      "| Epoch [  3/100] Iter[ 40/ 47]\t CE-loss: 0.5246\n",
      "| Epoch [  3/100] Iter[ 47/ 47]\t CE-loss: 0.4715\n",
      "Now evaluating on fold 0\n",
      "\n",
      "| Test Epoch #3\t Accuracy: 79.62%\t Loss: 0.03\t AUC: 0.891\t SPE: 0.893\n",
      "\n",
      "\n",
      "| Test Epoch #3\t Accuracy: 78.31%\t Loss: 0.04\t AUC: 0.841\t SPE: 0.841\n",
      "\n",
      "tensor([0.5000, 0.5000, 0.5000, 0.5000, 0.5000], device='cuda:0')\n",
      "saving model\n",
      "saving model\n",
      "saving model\n",
      "Now fold=0, best_acc=78.3132530120482， best_auc=0.8410714285714285, best_loss=0.03580554242593696\n",
      "| Epoch [  4/100] Iter[ 10/ 47]\t CE-loss: 0.4538\n",
      "| Epoch [  4/100] Iter[ 20/ 47]\t CE-loss: 0.6346\n",
      "| Epoch [  4/100] Iter[ 30/ 47]\t CE-loss: 0.2684\n",
      "| Epoch [  4/100] Iter[ 40/ 47]\t CE-loss: 0.3471\n",
      "| Epoch [  4/100] Iter[ 47/ 47]\t CE-loss: 0.3861\n",
      "Now evaluating on fold 0\n",
      "\n",
      "| Test Epoch #4\t Accuracy: 78.28%\t Loss: 0.03\t AUC: 0.907\t SPE: 0.909\n",
      "\n",
      "\n",
      "| Test Epoch #4\t Accuracy: 77.11%\t Loss: 0.04\t AUC: 0.881\t SPE: 0.879\n",
      "\n",
      "tensor([0.5000, 0.5000, 0.5000, 0.5000, 0.5000], device='cuda:0')\n",
      "saving model\n",
      "Now fold=0, best_acc=78.3132530120482， best_auc=0.880952380952381, best_loss=0.03580554242593696\n",
      "| Epoch [  5/100] Iter[ 10/ 47]\t CE-loss: 0.3997\n",
      "| Epoch [  5/100] Iter[ 20/ 47]\t CE-loss: 0.4410\n",
      "| Epoch [  5/100] Iter[ 30/ 47]\t CE-loss: 0.4709\n",
      "| Epoch [  5/100] Iter[ 40/ 47]\t CE-loss: 0.2907\n",
      "| Epoch [  5/100] Iter[ 47/ 47]\t CE-loss: 0.5508\n",
      "Now evaluating on fold 0\n",
      "\n",
      "| Test Epoch #5\t Accuracy: 76.27%\t Loss: 0.03\t AUC: 0.894\t SPE: 0.894\n",
      "\n",
      "\n",
      "| Test Epoch #5\t Accuracy: 68.67%\t Loss: 0.05\t AUC: 0.802\t SPE: 0.799\n",
      "\n",
      "tensor([0.5000, 0.5000, 0.5000, 0.5000, 0.5000], device='cuda:0')\n",
      "Now fold=0, best_acc=78.3132530120482， best_auc=0.880952380952381, best_loss=0.03580554242593696\n",
      "| Epoch [  6/100] Iter[ 10/ 47]\t CE-loss: 0.5102\n",
      "| Epoch [  6/100] Iter[ 20/ 47]\t CE-loss: 0.4183\n",
      "| Epoch [  6/100] Iter[ 30/ 47]\t CE-loss: 0.1728\n",
      "| Epoch [  6/100] Iter[ 40/ 47]\t CE-loss: 0.3954\n",
      "| Epoch [  6/100] Iter[ 47/ 47]\t CE-loss: 0.1923\n",
      "Now evaluating on fold 0\n",
      "\n",
      "| Test Epoch #6\t Accuracy: 78.42%\t Loss: 0.03\t AUC: 0.923\t SPE: 0.925\n",
      "\n",
      "\n",
      "| Test Epoch #6\t Accuracy: 68.67%\t Loss: 0.04\t AUC: 0.885\t SPE: 0.880\n",
      "\n",
      "tensor([0.5000, 0.5000, 0.5000, 0.5000, 0.5000], device='cuda:0')\n",
      "saving model\n",
      "Now fold=0, best_acc=78.3132530120482， best_auc=0.8851190476190476, best_loss=0.03580554242593696\n",
      "| Epoch [  7/100] Iter[ 10/ 47]\t CE-loss: 0.3671\n",
      "| Epoch [  7/100] Iter[ 20/ 47]\t CE-loss: 0.6590\n",
      "| Epoch [  7/100] Iter[ 30/ 47]\t CE-loss: 0.6153\n",
      "| Epoch [  7/100] Iter[ 40/ 47]\t CE-loss: 0.4190\n",
      "| Epoch [  7/100] Iter[ 47/ 47]\t CE-loss: 0.4044\n",
      "Now evaluating on fold 0\n",
      "\n",
      "| Test Epoch #7\t Accuracy: 80.43%\t Loss: 0.03\t AUC: 0.925\t SPE: 0.928\n",
      "\n",
      "\n",
      "| Test Epoch #7\t Accuracy: 72.29%\t Loss: 0.04\t AUC: 0.883\t SPE: 0.883\n",
      "\n",
      "tensor([0.5000, 0.5000, 0.5000, 0.5000, 0.5000], device='cuda:0')\n",
      "Now fold=0, best_acc=78.3132530120482， best_auc=0.8851190476190476, best_loss=0.03580554242593696\n",
      "| Epoch [  8/100] Iter[ 10/ 47]\t CE-loss: 0.3486\n",
      "| Epoch [  8/100] Iter[ 20/ 47]\t CE-loss: 0.3280\n",
      "| Epoch [  8/100] Iter[ 30/ 47]\t CE-loss: 0.6194\n",
      "| Epoch [  8/100] Iter[ 40/ 47]\t CE-loss: 0.3990\n",
      "| Epoch [  8/100] Iter[ 47/ 47]\t CE-loss: 0.2610\n",
      "Now evaluating on fold 0\n",
      "\n",
      "| Test Epoch #8\t Accuracy: 88.07%\t Loss: 0.02\t AUC: 0.949\t SPE: 0.950\n",
      "\n",
      "\n",
      "| Test Epoch #8\t Accuracy: 84.34%\t Loss: 0.03\t AUC: 0.896\t SPE: 0.890\n",
      "\n",
      "tensor([0.5000, 0.5000, 0.5000, 0.5000, 0.5000], device='cuda:0')\n",
      "saving model\n",
      "saving model\n",
      "saving model\n",
      "Now fold=0, best_acc=84.33734939759036， best_auc=0.8958333333333333, best_loss=0.030264387647789645\n",
      "| Epoch [  9/100] Iter[ 10/ 47]\t CE-loss: 0.3905\n",
      "| Epoch [  9/100] Iter[ 20/ 47]\t CE-loss: 0.2806\n",
      "| Epoch [  9/100] Iter[ 30/ 47]\t CE-loss: 0.2832\n",
      "| Epoch [  9/100] Iter[ 40/ 47]\t CE-loss: 0.2566\n",
      "| Epoch [  9/100] Iter[ 47/ 47]\t CE-loss: 0.2759\n",
      "Now evaluating on fold 0\n",
      "\n",
      "| Test Epoch #9\t Accuracy: 82.98%\t Loss: 0.02\t AUC: 0.937\t SPE: 0.939\n",
      "\n",
      "\n",
      "| Test Epoch #9\t Accuracy: 78.31%\t Loss: 0.03\t AUC: 0.896\t SPE: 0.895\n",
      "\n",
      "tensor([0.5000, 0.5000, 0.5000, 0.5000, 0.5000], device='cuda:0')\n",
      "saving model\n",
      "Now fold=0, best_acc=84.33734939759036， best_auc=0.8964285714285714, best_loss=0.030264387647789645\n",
      "| Epoch [ 10/100] Iter[ 10/ 47]\t CE-loss: 0.4380\n",
      "| Epoch [ 10/100] Iter[ 20/ 47]\t CE-loss: 0.3261\n",
      "| Epoch [ 10/100] Iter[ 30/ 47]\t CE-loss: 0.4774\n",
      "| Epoch [ 10/100] Iter[ 40/ 47]\t CE-loss: 0.2929\n",
      "| Epoch [ 10/100] Iter[ 47/ 47]\t CE-loss: 0.5310\n",
      "Now evaluating on fold 0\n",
      "\n",
      "| Test Epoch #10\t Accuracy: 88.34%\t Loss: 0.02\t AUC: 0.962\t SPE: 0.961\n",
      "\n",
      "\n",
      "| Test Epoch #10\t Accuracy: 79.52%\t Loss: 0.03\t AUC: 0.893\t SPE: 0.888\n",
      "\n",
      "tensor([0.5000, 0.5000, 0.5000, 0.5000, 0.5000], device='cuda:0')\n",
      "saving model\n",
      "Now fold=0, best_acc=84.33734939759036， best_auc=0.8964285714285714, best_loss=0.029147323983979512\n",
      "| Epoch [ 11/100] Iter[ 10/ 47]\t CE-loss: 0.1523\n",
      "| Epoch [ 11/100] Iter[ 20/ 47]\t CE-loss: 0.2380\n",
      "| Epoch [ 11/100] Iter[ 30/ 47]\t CE-loss: 0.4794\n",
      "| Epoch [ 11/100] Iter[ 40/ 47]\t CE-loss: 0.2535\n",
      "| Epoch [ 11/100] Iter[ 47/ 47]\t CE-loss: 0.3714\n",
      "Now evaluating on fold 0\n",
      "\n",
      "| Test Epoch #11\t Accuracy: 82.84%\t Loss: 0.02\t AUC: 0.946\t SPE: 0.947\n",
      "\n",
      "\n",
      "| Test Epoch #11\t Accuracy: 81.93%\t Loss: 0.03\t AUC: 0.911\t SPE: 0.908\n",
      "\n",
      "tensor([0.5000, 0.5000, 0.5000, 0.5000, 0.5000], device='cuda:0')\n",
      "saving model\n",
      "Now fold=0, best_acc=84.33734939759036， best_auc=0.9107142857142857, best_loss=0.029147323983979512\n",
      "| Epoch [ 12/100] Iter[ 10/ 47]\t CE-loss: 0.4835\n",
      "| Epoch [ 12/100] Iter[ 20/ 47]\t CE-loss: 0.1998\n",
      "| Epoch [ 12/100] Iter[ 30/ 47]\t CE-loss: 0.5239\n",
      "| Epoch [ 12/100] Iter[ 40/ 47]\t CE-loss: 0.3228\n",
      "| Epoch [ 12/100] Iter[ 47/ 47]\t CE-loss: 0.2412\n",
      "Now evaluating on fold 0\n",
      "\n",
      "| Test Epoch #12\t Accuracy: 83.11%\t Loss: 0.02\t AUC: 0.945\t SPE: 0.947\n",
      "\n",
      "\n",
      "| Test Epoch #12\t Accuracy: 78.31%\t Loss: 0.04\t AUC: 0.870\t SPE: 0.867\n",
      "\n",
      "tensor([0.5000, 0.5000, 0.5000, 0.5000, 0.5000], device='cuda:0')\n",
      "Now fold=0, best_acc=84.33734939759036， best_auc=0.9107142857142857, best_loss=0.029147323983979512\n",
      "| Epoch [ 13/100] Iter[ 10/ 47]\t CE-loss: 0.2479\n",
      "| Epoch [ 13/100] Iter[ 20/ 47]\t CE-loss: 0.3084\n",
      "| Epoch [ 13/100] Iter[ 30/ 47]\t CE-loss: 0.5096\n",
      "| Epoch [ 13/100] Iter[ 40/ 47]\t CE-loss: 0.4606\n",
      "| Epoch [ 13/100] Iter[ 47/ 47]\t CE-loss: 0.4854\n",
      "Now evaluating on fold 0\n",
      "\n",
      "| Test Epoch #13\t Accuracy: 90.21%\t Loss: 0.02\t AUC: 0.967\t SPE: 0.969\n",
      "\n",
      "\n",
      "| Test Epoch #13\t Accuracy: 80.72%\t Loss: 0.03\t AUC: 0.880\t SPE: 0.882\n",
      "\n",
      "tensor([0.5000, 0.5000, 0.5000, 0.5000, 0.5000], device='cuda:0')\n",
      "Now fold=0, best_acc=84.33734939759036， best_auc=0.9107142857142857, best_loss=0.029147323983979512\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Epoch [ 14/100] Iter[ 10/ 47]\t CE-loss: 0.2029\n",
      "| Epoch [ 14/100] Iter[ 20/ 47]\t CE-loss: 0.3359\n",
      "| Epoch [ 14/100] Iter[ 30/ 47]\t CE-loss: 0.2573\n",
      "| Epoch [ 14/100] Iter[ 40/ 47]\t CE-loss: 0.5053\n",
      "| Epoch [ 14/100] Iter[ 47/ 47]\t CE-loss: 0.1822\n",
      "Now evaluating on fold 0\n",
      "\n",
      "| Test Epoch #14\t Accuracy: 90.75%\t Loss: 0.02\t AUC: 0.965\t SPE: 0.967\n",
      "\n",
      "\n",
      "| Test Epoch #14\t Accuracy: 81.93%\t Loss: 0.03\t AUC: 0.884\t SPE: 0.886\n",
      "\n",
      "tensor([0.5000, 0.5000, 0.5000, 0.5000, 0.5000], device='cuda:0')\n",
      "Now fold=0, best_acc=84.33734939759036， best_auc=0.9107142857142857, best_loss=0.029147323983979512\n",
      "| Epoch [ 15/100] Iter[ 10/ 47]\t CE-loss: 0.2305\n",
      "| Epoch [ 15/100] Iter[ 20/ 47]\t CE-loss: 0.4580\n",
      "| Epoch [ 15/100] Iter[ 30/ 47]\t CE-loss: 0.2900\n",
      "| Epoch [ 15/100] Iter[ 40/ 47]\t CE-loss: 0.4794\n",
      "| Epoch [ 15/100] Iter[ 47/ 47]\t CE-loss: 0.2624\n",
      "Now evaluating on fold 0\n",
      "\n",
      "| Test Epoch #15\t Accuracy: 92.63%\t Loss: 0.01\t AUC: 0.977\t SPE: 0.978\n",
      "\n",
      "\n",
      "| Test Epoch #15\t Accuracy: 89.16%\t Loss: 0.03\t AUC: 0.919\t SPE: 0.923\n",
      "\n",
      "tensor([0.5000, 0.5000, 0.5000, 0.5000, 0.5000], device='cuda:0')\n",
      "saving model\n",
      "saving model\n",
      "saving model\n",
      "Now fold=0, best_acc=89.1566265060241， best_auc=0.9190476190476191, best_loss=0.02839928290930139\n",
      "| Epoch [ 16/100] Iter[ 10/ 47]\t CE-loss: 0.3181\n",
      "| Epoch [ 16/100] Iter[ 20/ 47]\t CE-loss: 0.3864\n",
      "| Epoch [ 16/100] Iter[ 30/ 47]\t CE-loss: 0.1374\n",
      "| Epoch [ 16/100] Iter[ 40/ 47]\t CE-loss: 0.5199\n",
      "| Epoch [ 16/100] Iter[ 47/ 47]\t CE-loss: 0.4542\n",
      "Now evaluating on fold 0\n",
      "\n",
      "| Test Epoch #16\t Accuracy: 92.90%\t Loss: 0.01\t AUC: 0.980\t SPE: 0.981\n",
      "\n",
      "\n",
      "| Test Epoch #16\t Accuracy: 81.93%\t Loss: 0.03\t AUC: 0.920\t SPE: 0.926\n",
      "\n",
      "tensor([0.5000, 0.5000, 0.5000, 0.5000, 0.5000], device='cuda:0')\n",
      "saving model\n",
      "saving model\n",
      "Now fold=0, best_acc=89.1566265060241， best_auc=0.9196428571428572, best_loss=0.027876606009092676\n",
      "| Epoch [ 17/100] Iter[ 10/ 47]\t CE-loss: 0.5047\n",
      "| Epoch [ 17/100] Iter[ 20/ 47]\t CE-loss: 0.3505\n",
      "| Epoch [ 17/100] Iter[ 30/ 47]\t CE-loss: 0.1324\n",
      "| Epoch [ 17/100] Iter[ 40/ 47]\t CE-loss: 0.5379\n",
      "| Epoch [ 17/100] Iter[ 47/ 47]\t CE-loss: 0.3018\n",
      "Now evaluating on fold 0\n",
      "\n",
      "| Test Epoch #17\t Accuracy: 93.57%\t Loss: 0.01\t AUC: 0.980\t SPE: 0.982\n",
      "\n",
      "\n",
      "| Test Epoch #17\t Accuracy: 86.75%\t Loss: 0.03\t AUC: 0.918\t SPE: 0.922\n",
      "\n",
      "tensor([0.5000, 0.5000, 0.5000, 0.5000, 0.5000], device='cuda:0')\n",
      "saving model\n",
      "Now fold=0, best_acc=89.1566265060241， best_auc=0.9196428571428572, best_loss=0.026709352092570568\n",
      "| Epoch [ 18/100] Iter[ 10/ 47]\t CE-loss: 0.1772\n",
      "| Epoch [ 18/100] Iter[ 20/ 47]\t CE-loss: 0.2719\n",
      "| Epoch [ 18/100] Iter[ 30/ 47]\t CE-loss: 0.1549\n",
      "| Epoch [ 18/100] Iter[ 40/ 47]\t CE-loss: 0.2924\n",
      "| Epoch [ 18/100] Iter[ 47/ 47]\t CE-loss: 0.3408\n",
      "Now evaluating on fold 0\n",
      "\n",
      "| Test Epoch #18\t Accuracy: 94.77%\t Loss: 0.01\t AUC: 0.984\t SPE: 0.985\n",
      "\n",
      "\n",
      "| Test Epoch #18\t Accuracy: 87.95%\t Loss: 0.03\t AUC: 0.928\t SPE: 0.930\n",
      "\n",
      "tensor([0.5000, 0.5000, 0.5000, 0.5000, 0.5000], device='cuda:0')\n",
      "saving model\n",
      "saving model\n",
      "Now fold=0, best_acc=89.1566265060241， best_auc=0.9279761904761905, best_loss=0.025563675057457155\n",
      "| Epoch [ 19/100] Iter[ 10/ 47]\t CE-loss: 0.1906\n",
      "| Epoch [ 19/100] Iter[ 20/ 47]\t CE-loss: 0.1791\n",
      "| Epoch [ 19/100] Iter[ 30/ 47]\t CE-loss: 0.2010\n",
      "| Epoch [ 19/100] Iter[ 40/ 47]\t CE-loss: 0.6048\n",
      "| Epoch [ 19/100] Iter[ 47/ 47]\t CE-loss: 0.2606\n",
      "Now evaluating on fold 0\n",
      "\n",
      "| Test Epoch #19\t Accuracy: 94.37%\t Loss: 0.01\t AUC: 0.985\t SPE: 0.987\n",
      "\n",
      "\n",
      "| Test Epoch #19\t Accuracy: 87.95%\t Loss: 0.03\t AUC: 0.918\t SPE: 0.927\n",
      "\n",
      "tensor([0.5000, 0.5000, 0.5000, 0.5000, 0.5000], device='cuda:0')\n",
      "saving model\n",
      "Now fold=0, best_acc=89.1566265060241， best_auc=0.9279761904761905, best_loss=0.02544871960059706\n",
      "| Epoch [ 20/100] Iter[ 10/ 47]\t CE-loss: 0.2370\n",
      "| Epoch [ 20/100] Iter[ 20/ 47]\t CE-loss: 0.4911\n",
      "| Epoch [ 20/100] Iter[ 30/ 47]\t CE-loss: 0.1830\n",
      "| Epoch [ 20/100] Iter[ 40/ 47]\t CE-loss: 0.2607\n",
      "| Epoch [ 20/100] Iter[ 47/ 47]\t CE-loss: 0.3418\n",
      "Now evaluating on fold 0\n",
      "\n",
      "| Test Epoch #20\t Accuracy: 94.37%\t Loss: 0.01\t AUC: 0.987\t SPE: 0.989\n",
      "\n",
      "\n",
      "| Test Epoch #20\t Accuracy: 84.34%\t Loss: 0.03\t AUC: 0.922\t SPE: 0.925\n",
      "\n",
      "tensor([0.5000, 0.5000, 0.5000, 0.5000, 0.5000], device='cuda:0')\n",
      "Now fold=0, best_acc=89.1566265060241， best_auc=0.9279761904761905, best_loss=0.02544871960059706\n",
      "| Epoch [ 21/100] Iter[ 10/ 47]\t CE-loss: 0.1548\n",
      "| Epoch [ 21/100] Iter[ 20/ 47]\t CE-loss: 0.3940\n",
      "| Epoch [ 21/100] Iter[ 30/ 47]\t CE-loss: 0.1768\n",
      "| Epoch [ 21/100] Iter[ 40/ 47]\t CE-loss: 0.0913\n",
      "| Epoch [ 21/100] Iter[ 47/ 47]\t CE-loss: 0.2938\n",
      "Now evaluating on fold 0\n",
      "\n",
      "| Test Epoch #21\t Accuracy: 95.31%\t Loss: 0.01\t AUC: 0.989\t SPE: 0.990\n",
      "\n",
      "\n",
      "| Test Epoch #21\t Accuracy: 89.16%\t Loss: 0.03\t AUC: 0.920\t SPE: 0.927\n",
      "\n",
      "tensor([0.5000, 0.5000, 0.5000, 0.5000, 0.5000], device='cuda:0')\n",
      "saving model\n",
      "Now fold=0, best_acc=89.1566265060241， best_auc=0.9279761904761905, best_loss=0.02544871960059706\n",
      "| Epoch [ 22/100] Iter[ 10/ 47]\t CE-loss: 0.3221\n",
      "| Epoch [ 22/100] Iter[ 20/ 47]\t CE-loss: 0.1556\n",
      "| Epoch [ 22/100] Iter[ 30/ 47]\t CE-loss: 0.1638\n",
      "| Epoch [ 22/100] Iter[ 40/ 47]\t CE-loss: 0.2105\n",
      "| Epoch [ 22/100] Iter[ 47/ 47]\t CE-loss: 0.3476\n",
      "Now evaluating on fold 0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-536c9886779e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Now evaluating on fold {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi_fold\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m             \u001b[0;34m(\u001b[0m\u001b[0macc_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mauc_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspe_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtrain_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_hat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_score\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNet_val\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_eval_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0;34m(\u001b[0m\u001b[0macc_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mauc_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspe_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtest_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_hat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_score\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNet_val\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-b4130de3eb26>\u001b[0m in \u001b[0;36mNet_val\u001b[0;34m(epoch, net, dataloader)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    559\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 561\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    562\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-0a75a114c836>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    128\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mimg_fill\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mimg1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m                 \u001b[0mimg_fill\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg_fill\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m                 \u001b[0mimg_fill\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresize_np\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_fill\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m                 \u001b[0moutput_img\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg_fill\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m                 \u001b[0mn\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-0a75a114c836>\u001b[0m in \u001b[0;36mresize_np\u001b[0;34m(img, img_new)\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0ma_new\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg_new\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m             \u001b[0mc_new\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg_new\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m             img = scipy.ndimage.zoom(\n\u001b[0m\u001b[1;32m     75\u001b[0m                 img, (a_new/a,  c_new/c))\n\u001b[1;32m     76\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/scipy/ndimage/interpolation.py\u001b[0m in \u001b[0;36mzoom\u001b[0;34m(input, zoom, output, order, mode, cval, prefilter)\u001b[0m\n\u001b[1;32m    637\u001b[0m     \u001b[0mmode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_ni_support\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_mode_to_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    638\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mprefilter\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0morder\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 639\u001b[0;31m         \u001b[0mfiltered\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspline_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    640\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    641\u001b[0m         \u001b[0mfiltered\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/scipy/ndimage/interpolation.py\u001b[0m in \u001b[0;36mspline_filter\u001b[0;34m(input, order, output, mode)\u001b[0m\n\u001b[1;32m    174\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0morder\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 176\u001b[0;31m             \u001b[0mspline_filter1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    177\u001b[0m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/scipy/ndimage/interpolation.py\u001b[0m in \u001b[0;36mspline_filter1d\u001b[0;34m(input, order, axis, output, mode)\u001b[0m\n\u001b[1;32m    126\u001b[0m         \u001b[0mmode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_ni_support\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_mode_to_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m         \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnormalize_axis_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m         \u001b[0m_nd_image\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspline_filter1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "## # 分层K折交叉验证\n",
    "filepath_df = pd.read_csv('../All_keep_v3/label_change6.csv')\n",
    "filepath_list = filepath_df.path\n",
    "label_list = filepath_df.label\n",
    "aug = 0.6\n",
    "ACC_score = [91.566, 89.157, 87.952, 84.337, 86.747,\n",
    "            83.133, 84.337, 85.542, 91.566, 86.585]\n",
    "AUC_score = [0.943, 0.939, 0.934, 0.922, 0.923,\n",
    "            0.869, 0.893, 0.925, 0.955, 0.918]\n",
    "LOSS_score = [1.662, 2.179, 2.077, 2.372, 2.462,\n",
    "             4.040, 2.544, 2.661, 2.215, 2.124]\n",
    "\n",
    "history = {'train_acc':[],\n",
    "           'test_acc':[],\n",
    "           'train_loss':[],\n",
    "           'test_loss':[]}\n",
    "\n",
    "skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "for i_fold, (train_index,\n",
    "        test_index) in enumerate(skf.split(X=filepath_list, y=label_list)):\n",
    "    if i_fold >= 0:\n",
    "        # 数据集划分\n",
    "        # X:\n",
    "        train_filepath_list = [filepath_list[idx] for idx in train_index]\n",
    "        test_filepath_list = [filepath_list[idx] for idx in test_index]\n",
    "        # y:\n",
    "        train_label_list = [label_list[idx] for idx in train_index]\n",
    "        test_label_list = [label_list[idx] for idx in test_index]\n",
    "\n",
    "        # dataset object\n",
    "        train_dataset = M1_dataset(path_list=train_filepath_list,\n",
    "                                   label_list=train_label_list, mode='train', aug=aug)\n",
    "        test_dataset = M1_dataset(path_list=test_filepath_list,\n",
    "                                  label_list=test_label_list, mode='test')\n",
    "        train_eval_dataset = M1_dataset(path_list=train_filepath_list,\n",
    "                               label_list=train_label_list, mode='eval')\n",
    "\n",
    "        # dataloader\n",
    "        train_eval_loader = DataLoader(train_eval_dataset, batch_size=16, shuffle=False)\n",
    "        train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "        print('Building dataloader: training: {}, testing: {}\\n'.format(len(train_dataset), len(test_dataset)))\n",
    "\n",
    "        # model initialize\n",
    "        model = Resmode(input_channel=5).to(args.device)\n",
    "        optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=0.0001, weight_decay=0.0001)\n",
    "        best_acc = 0\n",
    "        best_auc = 0\n",
    "        best_loss = 5\n",
    "\n",
    "        for epoch in range(args.max_epoch):\n",
    "            lr = 0.0001 * (0.1 ** int(epoch >= 15)) * (0.1 ** int(epoch >= 40))\n",
    "            for param_group in optimizer.param_groups:\n",
    "                param_group['lr'] = lr\n",
    "\n",
    "            train_simple(epoch, model, optimizer, train_loader)\n",
    "\n",
    "            print('Now evaluating on fold {}'.format(i_fold))\n",
    "            (acc_train, loss_train, auc_train, spe_train), (train_true, train_hat, train_score) = Net_val(epoch, model, train_eval_loader)\n",
    "\n",
    "            (acc_test, loss_test, auc_test, spe_test), (test_true, test_hat, test_score) = Net_val(epoch, model, test_loader)\n",
    "\n",
    "            history['train_acc'].append(acc_train)\n",
    "            history['test_acc'].append(acc_test)\n",
    "            history['train_loss'].append(loss_train)\n",
    "            history['test_loss'].append(loss_test)\n",
    "            print(model.get_gates())\n",
    "            \n",
    "    #         if auc_train >= auc_test:                                \n",
    "            if best_acc <= acc_test:\n",
    "                best_acc = acc_test\n",
    "                if best_acc > 70:\n",
    "                    torch.save(model.state_dict(), '../log4/model_OMv7_fold{}_acc0208.pth'.format(i_fold))\n",
    "                    print('saving model')\n",
    "\n",
    "            if best_auc <= auc_test:\n",
    "                best_auc = auc_test\n",
    "                if best_auc > 0.8:\n",
    "                    torch.save(model.state_dict(), '../log4/model_OMv7_fold{}_auc0208.pth'.format(i_fold))\n",
    "                    best_gate = F.softmax(model.get_gates(), dim=-1)\n",
    "                    best_gate = np.array(best_gate.detach().cpu())\n",
    "                    print('saving model')\n",
    "\n",
    "            if best_loss >= loss_test:\n",
    "                best_loss = loss_test\n",
    "                if best_loss < 5:\n",
    "                    torch.save(model.state_dict(), '../log4/model_OMv7_fold{}_loss0208.pth'.format(i_fold))\n",
    "                    print('saving model')\n",
    "\n",
    "            print('Now fold={}, best_acc={}， best_auc={}, best_loss={}'\n",
    "                  .format(i_fold, best_acc, best_auc, best_loss) )\n",
    "\n",
    "            if auc_train >=0.9 and aug < 0.8:\n",
    "                aug += 0.5\n",
    "                train_dataset = M1_dataset(path_list=train_filepath_list,\n",
    "                                   label_list=train_label_list, mode='train', aug=aug)\n",
    "                train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "            if auc_train > 0.995:\n",
    "                break\n",
    "        \n",
    "        T1 = time.time()\n",
    "        log = 'Exp on 20230208 OnlyMaskResnet ------------------------------------------------------\\n'\n",
    "        log += 'Fold: %d, loss: %.4f, acc: %.4f, auc: %.4f, time: %s\\n' % (\n",
    "            i_fold, best_loss, best_acc, best_auc, time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime()))\n",
    "        log += 'saved_gate: {}\\n'.format(best_gate)\n",
    "        f = open('../log4/log_OM.txt', 'a')\n",
    "        f.write(log)\n",
    "        f.close()\n",
    "        \n",
    "np.save('trainig_history_mask.npy', history) # 注意带上后缀名\n"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "jupytext": {
   "cell_metadata_filter": "-all",
   "encoding": "# coding: utf-8",
   "executable": "/usr/bin/env python",
   "main_language": "python",
   "notebook_metadata_filter": "-all",
   "text_representation": {
    "extension": ".py",
    "format_name": "percent"
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
